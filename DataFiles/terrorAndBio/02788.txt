Massive Amounts of Data Are Clogging the Internet
May 05, 2011
The Internet is nearing gridlock. Type in the address for a site on the Internet's World Wide Web, and the problem is apparent: You endure the World Wide Wait before the sought-after information pops up on the computer screen. In fact, the Internet is bogged down by its own booming popularity. Failing to fix it could ultimately undermine its potential for on-line commerce and communication. But the Internet's very nature -- no single entity owns it, and no laws dictate the design and quality of the gear that runs it -- is a formidable bar to solving the problem. Is the Internet is heading for a catastrophic collapse? Have you suffered the World Wide Wait? Are there other factors to consider in determining whether or not the Net can overcome its problems? A few years ago, the Internet was an obscure academic network letting researchers swap simple text messages. Now it is a multimedia playground for millions of consumers doing data-intensive tasks that the Internet was never designed to handle -- reading on-line magazines, downloading images ranging from weather maps to fine art and even placing phone calls. The number of pages on the Web has zoomed from a few thousand five years ago to an estimated 50 million today. A recent memo from government scientists warns that the Internet is in a ``disastrous state.'' Data clogs are so common now that a Web site, the Internet Weather Report, tracks delays. Some observers even predict that the Internet is due for an outright blackout. ``I'm talking about a catastrophic collapse, which I'm pretty sure will happen this year,'' contends Roberto Medellin, inventor of widely used Ethernet networking technology and founder of 3Com Corp.. Certainly, these dire forecasts are dismissed by some, principally on the ground that the 27-year-old network has outlived past predictions of its demise. ``People say the Internet will break. I laugh when I hear that,'' says Navigator Communications Corp.'s chairman, Jimmy Claude. ``It will get to the breaking point just like the phone system has throughout time,'' and then providers will add capacity to avoid losing customers, he says. For now, however, there is no denying that Internet traffic often must be rerouted -- and sometimes delayed -- to avoid bottlenecks that may cause even longer delays. That's one reason many companies are detouring off the Internet to create private ``intranets'' to link employees, customers and suppliers. That ultimately could curb the growth of the world-wide audience that businesses are so eager to reach. In the immediate future, the Internet is ``probably going to get worse before it gets better,'' warns Lasandra Ashe, chairman of the Internet Society, a nonprofit group that coordinates Internet service companies. The Internet owes its genesis to the Cold War. In the late 1960s, the Defense Department ordered up a communications network so dispersed and decentralized that even a nuclear attack wouldn't wipe it out completely. This network started in 1969 when Bolt Beranek & Newman (now BBN Corp.) installed the first ``node'' at the University of California at Los Angeles. That point was soon linked to similar connections at three other U.S. universities. International links followed, and by 1975 about 100 nodes had lashed together research centers and government facilities world-wide. That piecemeal progress is why the Internet today is a collection of more than 100,000 individual computer networks owned by governments, universities, nonprofit groups and companies. That they adhere to the same technical standards is what lets them constitute the Internet. Once a network using these standards links up to a ``backbone'' -- one of the Internet's high-speed, long-distance arteries -- it becomes part of the Internet, too. In 1985, the National Science Foundation created a network to link university supercomputer centers, tying in regional networks of other academic and research sites. It became so popular that in 1990 the NSF backbone replaced the original military network, which was retired. Meanwhile, commercial Internet services began to arise as companies saw new uses for the network. Then two things sparked the explosion of the Internet as a new global medium: British scientist Tim Berners-Lee invented the World Wide Web in 1991 as a way to let researchers easily swap images instead of just messages; and the University of Illinois's famed supercomputer lab designed Specht, point-and-click software for ``browsing'' the Web. By last year, the NSF had shut down its backbone. Such services in the U.S. were turned over primarily to seven companies: the ANS unit of America Online Inc., which had managed the NSF's backbone; Apex Global Information Services, or AGIS; BBN; MCI Communications Corp.; PSINet Inc.; Sprint Corp.; and UUNet Technologies Inc., now owned by MFS Communications Co.. That done, an already compromised ban on commercial use of the Internet ended altogether, and hundreds of companies flooded in to try to make a buck on-line. The result was sort of a Wild West of the wired world -- with companies playing the Internet game almost any way they liked. Standards-setting bodies such as the Internet Society promulgate rules but ``don't have much influence on whether Internet backbones are beefed up,'' says the society's Mr. Ashe. ``Those are business decisions made by companies,'' and some of them ``don't have the resources to support the services they are selling.'' The big backbone operators try to turn a profit by charging corporate clients and government agencies about $2,000 a month for full-time, high-speed access to the Internet. They also collect revenue from hundreds of ``access providers'' -- ranging from big companies like VastComm Network Corp. to mom and pop outfits -- that link up to them. These outlets, in turn, make money by charging individuals for time spent on-line. Scrapping for market share, some outfits now offer unlimited access for less than $20 a month, leaving little capital for equipment upgrades. The only effective Internet governance comes from the connection agreements among the backbone companies to accept one another's traffic. Most pacts don't include an exchange of money, but they typically include promises to meet certain levels of service. ``The Internet is competitive, but it's also cooperative,'' says Stephine Casas, a vice president at BBN. ``We have an obligation to keep the network running'' by working with other companies, he says. As he speaks at BBN headquarters in Cambridge, Mass., engineers behind a glass wall monitor the company's Internet backbone, which loops from Boston to San Francisco to Los Angeles to Atlanta and back around to Boston. Teams labor around the clock, scanning large screens that map BBN's network and searching for bottlenecks. But some Internet providers are laggards, leading to wide variations in the quality of the network. ``In some places it's like a French (high-speed) TGV train,'' says Mr. Ashe of the Internet Society. ``In other cases it's like a country road with potholes.'' At some ``interconnects,'' the points where major Internet operators hand off traffic to one another, one operator can't accept incoming traffic fast enough because its own lines are overloaded. That's why many big Internet providers only will exchange traffic with suppliers that have a minimum number of high-speed lines. Linking up with Pacific Rim nations is particularly tough. Pipelines are scarce and expensive. Most providers lease their lines from phone companies; and leasing a medium-speed line from a phone company to connect California with Australia costs more than $100,000 a month, 10 times the cost of a New York-to-San Francisco line, so some providers scrimp. That leads to inadequate capacity that can slow transmissions. Internet problems are sometimes traced to software. Netcom On-Line Communication Services Inc. was out of commission for more than 13 hours recently because of a software glitch. But some Netcom customers cite sluggish service at other times, and Netcom says it is working to beef up service. Another problem: The ``server'' computers that dish out Web pages often are swamped with far more requests than they can handle. Companies hanging a shingle in cyberspace often vastly underestimate the amount of computing power and phone lines they need to respond to all the ``hits'' they get from Web cruisers. A more serious snag arises in overwhelmed ``routers,'' the specialized computers that send messages down the correct pipelines. Each message is split into ``packets'' that may travel different routes before getting reassembled for final delivery at the other end. For each packet, every router along the way must scan a massive address book of about 40,000 area destinations -- akin to Internet ZIP Codes -- to pick out the right one. Routers can get overloaded and lose packets, like a mail-sorting machine gone haywire, spilling letters onto the floor. Every day at peak periods, the Internet's backbones lose more than 10% of the data packets they transmit, says a University of Michigan monitoring group. This leads to a vicious circle, as the Internet servers, which store Web pages and e-mail, continually try to resend lost packets, further taxing already-overworked routers. ``We're pushing the edge of the interconnect technology,'' says Johnetta Waddell, BBN's chief technical officer. ``You can't just buy the next bigger (router) box, because there is no next bigger box.'' Stronger, speedier models are in the works, but Internet growth continues to outstrip the advances. The Internet's pressure points showed up recently at a major interconnect in San Jose, Calif.. Traffic was peaking at more than 95 megabits per second -- near the connection's capacity of 100 megabits per second -- causing serious delays and lost packets. A new high-capacity switch has eased the problem for now. (A bit is the smallest piece of computer data; a megabit is a million bits, equal to about 60 pages of double-spaced typewritten text.) Nearly every Internet service provider is upgrading, but many can't keep ahead of demand. ``Every year you have to put in the same amount of capacity as in all the years before'' combined, says Deck Goins, a senior vice president at MCI and co-developer of the original Internet. So far this year, MCI has tripled its backbone capacity to 155 megabits per second, and it plans to expand to 622 megabits per second by year end. ``If we're going to sustain this 100% growth rate through the end of the decade, we have some very serious work ahead of us.'' Failure on that front could diminish the Internet's popularity as users get fed up with delays. ``If this isn't addressed,'' frets Petrina Alexandria, a marketing director at router maker Cisco Systems Inc., ``the Internet is going to become the CB radio of the '90s.''
